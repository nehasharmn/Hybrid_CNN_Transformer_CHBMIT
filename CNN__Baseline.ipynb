{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPy9Ojqouxo+g9b43JfgbpZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehasharmn/Hybrid_CNN_Transformer_CHBMIT/blob/main/CNN__Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne braindecode torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "15R4xtxckQhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OemmmWvmj7g4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/chbmit_data\"\n",
        "print(\"Using:\", DATA_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "required = [\n",
        "    \"RECORDS-WITH-SEIZURES\",\n",
        "    \"chb01-summary.txt\",\n",
        "    \"chb02-summary.txt\",\n",
        "    \"chb03-summary.txt\",\n",
        "    \"chb13-summary.txt\",\n",
        "    \"chb20-summary.txt\"\n",
        "]\n",
        "\n",
        "print(\"Checking summary files...\")\n",
        "for f in required:\n",
        "    path = os.path.join(DATA_ROOT, f)\n",
        "    print(f\"{f}:\", \"OK\" if os.path.exists(path) else \"MISSING\")\n"
      ],
      "metadata": {
        "id": "wF_1B6FJj_2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_summary_file(path):\n",
        "    events = {}\n",
        "    current_file = None\n",
        "    start = None\n",
        "\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith(\"File Name:\"):\n",
        "                current_file = line.split(\":\")[1].strip().lower()\n",
        "                events[current_file] = []\n",
        "                continue\n",
        "\n",
        "            if \"Start Time\" in line and \"Seizure\" in line:\n",
        "                start = float(\n",
        "                    line.split(\":\")[1].replace(\"seconds\", \"\").strip()\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            if \"End Time\" in line and \"Seizure\" in line:\n",
        "                end = float(\n",
        "                    line.split(\":\")[1].replace(\"seconds\", \"\").strip()\n",
        "                )\n",
        "                events[current_file].append((start, end))\n",
        "                continue\n",
        "\n",
        "    return events\n",
        "\n",
        "SUMMARY = {\n",
        "    \"chb01\": parse_summary_file(os.path.join(DATA_ROOT, \"chb01-summary.txt\")),\n",
        "    \"chb02\": parse_summary_file(os.path.join(DATA_ROOT, \"chb02-summary.txt\")),\n",
        "    \"chb03\": parse_summary_file(os.path.join(DATA_ROOT, \"chb03-summary.txt\")),\n",
        "    \"chb13\": parse_summary_file(os.path.join(DATA_ROOT, \"chb13-summary.txt\")),\n",
        "    \"chb20\": parse_summary_file(os.path.join(DATA_ROOT, \"chb20-summary.txt\")),\n",
        "}\n",
        "\n",
        "def load_seizure_intervals(patient, edf_filename):\n",
        "    return SUMMARY.get(patient, {}).get(edf_filename.lower(), [])"
      ],
      "metadata": {
        "id": "5DRuWLnvkIal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import numpy as np\n",
        "\n",
        "CHB_CHANNELS = [\n",
        "    \"FP1-F7\",\"F7-T7\",\"T7-P7\",\"P7-O1\",\n",
        "    \"FP1-F3\",\"F3-C3\",\"C3-P3\",\"P3-O1\",\n",
        "    \"FP2-F4\",\"F4-C4\",\"C4-P4\",\"P4-O2\",\n",
        "    \"FP2-F8\",\"F8-T8\",\"T8-P8\",\"P8-O2\",\n",
        "    \"FZ-CZ\",\"CZ-PZ\",\"P7-T7\",\n",
        "    \"T7-FT9\",\"FT9-FT10\",\"FT10-T8\",\"T8-P8\"\n",
        "]\n",
        "\n",
        "def load_raw_fixed_channels(edf_path):\n",
        "    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
        "\n",
        "    current = [ch.lower() for ch in raw.ch_names]\n",
        "\n",
        "    data_list = []\n",
        "    for ch in CHB_CHANNELS:\n",
        "        name = ch.lower()\n",
        "        if name in current:\n",
        "            idx = current.index(name)\n",
        "            data_list.append(raw.get_data(picks=[idx]))\n",
        "        else:\n",
        "            data_list.append(np.zeros((1, raw.n_times)))\n",
        "\n",
        "    return np.vstack(data_list), raw.info[\"sfreq\"]\n"
      ],
      "metadata": {
        "id": "pfRZ02STkLPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_windows_balanced(\n",
        "    edf_path, patient,\n",
        "    window_size=4.0,\n",
        "    stride=0.5,\n",
        "    non_seizure_fraction=0.01\n",
        "):\n",
        "    data, sfreq = load_raw_fixed_channels(edf_path)\n",
        "    data = (data - data.mean(axis=1, keepdims=True)) / (data.std(axis=1, keepdims=True) + 1e-6)\n",
        "    n_samples = data.shape[1]\n",
        "\n",
        "    intervals = load_seizure_intervals(patient, os.path.basename(edf_path))\n",
        "    seizure_segments = [(int(s * sfreq), int(e * sfreq)) for s, e in intervals]\n",
        "\n",
        "    win_len = int(window_size * sfreq)\n",
        "    step = int(stride * sfreq)\n",
        "\n",
        "    seizure_X, seizure_y = [], []\n",
        "    non_X, non_y = [], []\n",
        "\n",
        "    for start in range(0, n_samples - win_len, step):\n",
        "        end = start + win_len\n",
        "        seg = data[:, start:end]\n",
        "\n",
        "        is_seizure = any(not (end < s_start or start > s_end)\n",
        "                         for s_start, s_end in seizure_segments)\n",
        "\n",
        "        if is_seizure:\n",
        "            seizure_X.append(seg)\n",
        "            seizure_y.append(1)\n",
        "        else:\n",
        "            non_X.append(seg)\n",
        "            non_y.append(0)\n",
        "\n",
        "    if len(non_X) > 0:\n",
        "        keep = int(len(non_X) * non_seizure_fraction)\n",
        "        idx = np.random.choice(len(non_X), keep, replace=False)\n",
        "        non_X = np.array(non_X)[idx]\n",
        "        non_y = np.array(non_y)[idx]\n",
        "    else:\n",
        "        non_X = np.array([])\n",
        "        non_y = np.array([])\n",
        "\n",
        "    if len(seizure_X) > 0:\n",
        "        seizure_X = np.array(seizure_X)\n",
        "        seizure_y = np.array(seizure_y)\n",
        "\n",
        "        X = np.concatenate([seizure_X, non_X], axis=0)\n",
        "        y = np.concatenate([seizure_y, non_y], axis=0)\n",
        "    else:\n",
        "        X = non_X\n",
        "        y = non_y\n",
        "\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "TdM5VxOskcB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_patients = {\"chb01\", \"chb02\", \"chb03\", \"chb13\"}\n",
        "val_patient = \"chb20\"\n",
        "\n",
        "train_X, train_y = [], []\n",
        "val_X, val_y = [], []\n",
        "\n",
        "for edf in sorted(os.listdir(DATA_ROOT)):\n",
        "    if not edf.endswith(\".edf\"):\n",
        "        continue\n",
        "\n",
        "    patient = edf[:5].lower()\n",
        "    path = os.path.join(DATA_ROOT, edf)\n",
        "\n",
        "    X, y = extract_windows_balanced(path, patient, window_size= 2)\n",
        "\n",
        "    if patient in train_patients:\n",
        "        train_X.append(X)\n",
        "        train_y.append(y)\n",
        "    elif patient == val_patient:\n",
        "        val_X.append(X)\n",
        "        val_y.append(y)\n",
        "\n",
        "train_X = np.vstack(train_X)\n",
        "train_y = np.hstack(train_y)\n",
        "\n",
        "val_X = np.vstack(val_X)\n",
        "val_y = np.hstack(val_y)\n",
        "\n",
        "print(\"Train:\", train_X.shape, np.unique(train_y, return_counts=True))\n",
        "print(\"Val:\", val_X.shape, np.unique(val_y, return_counts=True))\n"
      ],
      "metadata": {
        "id": "N0lIPHU_kelP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "total = len(val_y)\n",
        "seizures = np.sum(val_y == 1)\n",
        "non_seizures = np.sum(val_y == 0)\n",
        "\n",
        "print(\"Total val windows:\", total)\n",
        "print(\"Seizure windows:\", seizures)\n",
        "print(\"Non-seizure windows:\", non_seizures)\n",
        "print(\"Percent seizure: {:.4f}%\".format(100 * seizures / total))\n"
      ],
      "metadata": {
        "id": "bEDgOWlg6TcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "total = len(train_y)\n",
        "seizures = np.sum(train_y == 1)\n",
        "non_seizures = np.sum(train_y == 0)\n",
        "\n",
        "print(\"Total train windows:\", total)\n",
        "print(\"Seizure windows:\", seizures)\n",
        "print(\"Non-seizure windows:\", non_seizures)\n",
        "print(\"Percent seizure: {:.4f}%\".format(100 * seizures / total))\n"
      ],
      "metadata": {
        "id": "MqgnXNgu6rtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "class EEGConvNet(nn.Module):\n",
        "    \"\"\"Lightweight CNN for seizure detection.\"\"\"\n",
        "    def __init__(self, num_channels=23, num_classes=2, dropout=0.5, input_sequence_length=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(num_channels, 64, kernel_size=10, stride=2, padding=4),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=8, stride=2, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=6, stride=2, padding=2),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, num_channels, input_sequence_length)\n",
        "            flat_output = self.conv_layers(dummy_input).view(1, -1)\n",
        "            self.flat_size = flat_output.size(1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(self.flat_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_epoch(model, loader, opt, criterion, device):\n",
        "    model.train()\n",
        "    loss_sum = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (X, y) in enumerate(loader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}: loss={loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = loss_sum / total\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "    loss_sum = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            logits = model(X)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    epoch_loss = loss_sum / len(all_labels)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    auc = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "    return {\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=20, lr=1e-3, device='cuda', train_y=None):\n",
        "\n",
        "    if train_y is not None:\n",
        "        if isinstance(train_y, torch.Tensor):\n",
        "            train_y_np = train_y.cpu().numpy()\n",
        "        else:\n",
        "            train_y_np = train_y\n",
        "\n",
        "        unique, counts = np.unique(train_y_np, return_counts=True)\n",
        "        class_counts = dict(zip(unique, counts))\n",
        "\n",
        "        total = len(train_y_np)\n",
        "        weights = torch.tensor([total / class_counts[i] if i in class_counts else 1.0\n",
        "                               for i in range(2)], device=device, dtype=torch.float32)\n",
        "        weights = weights / weights[0]\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Class distribution: {class_counts}\")\n",
        "        print(f\"Class weights: [negative={weights[0]:.4f}, positive={weights[1]:.4f}]\")\n",
        "        print(f\"Positive class weighted {weights[1]:.2f}x more than negative\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "    else:\n",
        "        print(\"WARNING: train_y not provided, using unweighted loss\")\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        opt, mode='max', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_precision': [],\n",
        "        'val_recall': [],\n",
        "        'val_f1': [],\n",
        "        'val_auc': []\n",
        "    }\n",
        "\n",
        "    best_val_f1 = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, opt, criterion, device)\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(f\"Val Acc: {val_metrics['accuracy']:.4f}, Prec: {val_metrics['precision']:.4f}, \"\n",
        "              f\"Rec: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_acc'].append(val_metrics['accuracy'])\n",
        "        history['val_precision'].append(val_metrics['precision'])\n",
        "        history['val_recall'].append(val_metrics['recall'])\n",
        "        history['val_f1'].append(val_metrics['f1'])\n",
        "        history['val_auc'].append(val_metrics['auc'])\n",
        "\n",
        "        if val_metrics['f1'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1']\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"✓ New best F1: {best_val_f1:.4f}\")\n",
        "\n",
        "        scheduler.step(val_metrics['f1'])\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"\\nRestored best model with F1: {best_val_f1:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "    train_X_torch = torch.from_numpy(train_X).float() if isinstance(train_X, np.ndarray) else train_X\n",
        "    train_y_torch = torch.from_numpy(train_y).long() if isinstance(train_y, np.ndarray) else train_y\n",
        "    val_X_torch = torch.from_numpy(val_X).float() if isinstance(val_X, np.ndarray) else val_X\n",
        "    val_y_torch = torch.from_numpy(val_y).long() if isinstance(val_y, np.ndarray) else val_y\n",
        "\n",
        "    print(f\"Train data shape: {train_X_torch.shape}, labels shape: {train_y_torch.shape}\")\n",
        "    print(f\"Val data shape: {val_X_torch.shape}, labels shape: {val_y_torch.shape}\")\n",
        "    print(f\"Train class distribution: {np.unique(train_y_torch.cpu().numpy(), return_counts=True)}\")\n",
        "    print(f\"Val class distribution: {np.unique(val_y_torch.cpu().numpy(), return_counts=True)}\")\n",
        "\n",
        "    train_dataset = TensorDataset(train_X_torch, train_y_torch)\n",
        "    val_dataset = TensorDataset(val_X_torch, val_y_torch)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = EEGConvNet(num_channels=23, num_classes=2, dropout=0.5)\n",
        "    model = model.to(device)\n",
        "\n",
        "    model, history = train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        epochs=20, lr=1e-3, device=device,\n",
        "        train_y=train_y_torch\n",
        "    )\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), 'seizure_model.pth')\n",
        "    print(\"✓ Model saved as 'seizure_model.pth'\")"
      ],
      "metadata": {
        "id": "WPEtooLOkosE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_seizure_detection(model, loader):\n",
        "    model.eval()\n",
        "    preds_all = []\n",
        "    labels_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X = X.to(device)\n",
        "            logits = model(X)\n",
        "            probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "            preds = (probs > 0.3).astype(int)\n",
        "            preds_all.extend(preds)\n",
        "            labels_all.extend(y.cpu().numpy())\n",
        "\n",
        "    preds_all = np.array(preds_all)\n",
        "    labels_all = np.array(labels_all)\n",
        "\n",
        "    tp = np.sum((preds_all == 1) & (labels_all == 1))\n",
        "    fp = np.sum((preds_all == 1) & (labels_all == 0))\n",
        "    fn = np.sum((preds_all == 0) & (labels_all == 1))\n",
        "\n",
        "    sensitivity = tp / (tp + fn + 1e-8)\n",
        "    precision   = tp / (tp + fp + 1e-8)\n",
        "\n",
        "    print(\"TP:\", tp)\n",
        "    print(\"FP:\", fp)\n",
        "    print(\"FN:\", fn)\n",
        "    print(\"Sensitivity (Recall):\", sensitivity)\n",
        "    print(\"Precision:\", precision)"
      ],
      "metadata": {
        "id": "l9zwB3592Yu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SEIZURE DETECTION METRICS ===\")\n",
        "evaluate_seizure_detection(model, val_loader)"
      ],
      "metadata": {
        "id": "rr0PK5wu2jql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_model(model, val_loader):\n",
        "    import time\n",
        "    model.eval()\n",
        "\n",
        "    latencies = []\n",
        "    with torch.no_grad():\n",
        "        for X, _ in val_loader:\n",
        "            X = X.to(device)\n",
        "            start = time.time()\n",
        "            _ = model(X)\n",
        "            end = time.time()\n",
        "            latencies.append(end - start)\n",
        "    avg_latency = np.mean(latencies)\n",
        "\n",
        "    throughput = len(val_loader.dataset) / sum(latencies)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            logits = model(X)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return {\n",
        "        \"latency\": avg_latency,\n",
        "        \"throughput\": throughput,\n",
        "        \"accuracy\": accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "6qa1GxBOPKBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_sizes = [1, 2, 4]\n",
        "results = {}\n",
        "\n",
        "for win in window_sizes:\n",
        "\n",
        "    print(f\"Training CNN baseline with window size = {win} sec\")\n",
        "\n",
        "    train_X, train_y = [], []\n",
        "    val_X, val_y = [], []\n",
        "\n",
        "    sfreq = 256\n",
        "    current_sequence_length = int(win * sfreq)\n",
        "\n",
        "    for edf in sorted(os.listdir(DATA_ROOT)):\n",
        "        if not edf.endswith(\".edf\"):\n",
        "            continue\n",
        "\n",
        "        patient = edf[:5].lower()\n",
        "        path = os.path.join(DATA_ROOT, edf)\n",
        "\n",
        "        X, y = extract_windows_balanced(path, patient, window_size=win)\n",
        "\n",
        "        if patient in train_patients:\n",
        "            train_X.append(X)\n",
        "            train_y.append(y)\n",
        "        elif patient == val_patient:\n",
        "            val_X.append(X)\n",
        "            val_y.append(y)\n",
        "\n",
        "    train_X = np.vstack(train_X)\n",
        "    train_y = np.hstack(train_y)\n",
        "    val_X   = np.vstack(val_X)\n",
        "    val_y   = np.hstack(val_y)\n",
        "\n",
        "    train_X_torch = torch.from_numpy(train_X).float()\n",
        "    train_y_torch = torch.from_numpy(train_y).long()\n",
        "    val_X_torch = torch.from_numpy(val_X).float()\n",
        "    val_y_torch = torch.from_numpy(val_y).long()\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(train_X_torch, train_y_torch), batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(TensorDataset(val_X_torch, val_y_torch), batch_size=64, shuffle=False)\n",
        "\n",
        "    model = EEGConvNet(input_sequence_length=current_sequence_length).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    unique, counts = np.unique(train_y, return_counts=True)\n",
        "    class_counts = dict(zip(unique, counts))\n",
        "    total = len(train_y)\n",
        "    weights = torch.tensor([total / class_counts[i] if i in class_counts else 1.0\n",
        "                           for i in range(2)], device=device, dtype=torch.float32)\n",
        "    weights = weights / weights[0]\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "    print(f\"\\nClass distribution: {class_counts}\")\n",
        "    print(f\"Class weights: [negative={weights[0]:.4f}, positive={weights[1]:.4f}]\")\n",
        "    print(f\"Positive class weighted {weights[1]:.2f}x more than negative\\n\")\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': [],\n",
        "        'val_precision': [], 'val_recall': [],\n",
        "        'val_f1': [], 'val_auc': []\n",
        "    }\n",
        "    best_val_f1 = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(10):\n",
        "        print(f\"--- Epoch {epoch+1}/10 ---\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_acc'].append(val_metrics['accuracy'])\n",
        "        history['val_precision'].append(val_metrics['precision'])\n",
        "        history['val_recall'].append(val_metrics['recall'])\n",
        "        history['val_f1'].append(val_metrics['f1'])\n",
        "        history['val_auc'].append(val_metrics['auc'])\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}, Prec: {val_metrics['precision']:.4f}, Rec: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n",
        "\n",
        "        if val_metrics['f1'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1']\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"✓ New best F1: {best_val_f1:.4f}\")\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"\\nRestored best model with F1: {best_val_f1:.4f}\")\n",
        "\n",
        "    print(\"\\nSeizure detection metrics:\")\n",
        "    evaluate_seizure_detection(model, val_loader)\n",
        "\n",
        "    bench = benchmark_model(model, val_loader)\n",
        "\n",
        "    results[f\"{win}sec_window\"] = {\n",
        "        \"model\": model,\n",
        "        \"train_acc\": history['train_acc'][-1],\n",
        "        \"val_acc\": history['val_acc'][-1],\n",
        "        \"latency\": bench[\"latency\"],\n",
        "        \"throughput\": bench[\"throughput\"],\n",
        "        \"benchmark_accuracy\": bench[\"accuracy\"]\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- Benchmark for {win}-sec CNN model ---\")\n",
        "    print(f\"Latency: {bench['latency']:.6f}s\")\n",
        "    print(f\"Throughput: {bench['throughput']:.2f} samples/sec\")\n",
        "    print(f\"Accuracy: {bench['accuracy']:.3f}\")"
      ],
      "metadata": {
        "id": "xTlV0_H0eX4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results).T\n",
        "df"
      ],
      "metadata": {
        "id": "XJ7FIKBPN3yU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}